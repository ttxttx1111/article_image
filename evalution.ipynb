{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from model import ImageCNN,MatchCNN\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "from data_loader import get_loader,CocoDataset\n",
    "from build_vocab import Vocabulary\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from random import shuffle\n",
    "from matchCNN_st import MatchCNN_st\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load coco dataset\"\"\"\n",
    "data_dir = \"../data/coco/\"\n",
    "annotation_file = data_dir + \"annotations/captions_train2014.json\"\n",
    "coco = COCO(annotation_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"randomly extract  imgid and corresponding  captionid\"\"\"\n",
    "sample_num = 100\n",
    "# caption_num = sample_num * 1000\n",
    "img_ids_all = list(coco.imgs.keys())\n",
    "shuffle(img_ids_all)\n",
    "img_ids = []\n",
    "ann_ids = []\n",
    "\n",
    "for key in img_ids_all:\n",
    "    temp = coco.getAnnIds(key)\n",
    "    ann_ids.append(temp[0])\n",
    "    img_ids.append(key)\n",
    "    if len(img_ids) == sample_num:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only 1-element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1d3f5c931cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/env/ttx/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__float__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         raise TypeError(\"only 1-element tensors can be converted \"\n\u001b[0m\u001b[1;32m    390\u001b[0m                         \"to Python scalars\")\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only 1-element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"preprocess images\"\"\"\n",
    "image_dir = data_dir + \"resized2014/\"\n",
    "imgs = []\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\"\"\"\n",
    "imgs:\n",
    "    list of img\n",
    "img:\n",
    "    ann_ids, data, id\n",
    "\"\"\"\n",
    "for i, img_id in enumerate(img_ids):\n",
    "    path = coco.loadImgs(img_id)[0]['file_name']\n",
    "    image = Image.open(os.path.join(image_dir, path)).convert('RGB')\n",
    "    if transform is not None:\n",
    "        image = transform(image)\n",
    "    imgs.append(image)\n",
    "imgs = np.array(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"preprocess annotations\"\"\"\n",
    "vocab_file = \"../data/coco/vocab.pkl\"\n",
    "pad_len = 62\n",
    "# Load vocabulary wrapper.\n",
    "with open(vocab_file, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "anns = np.zeros((sample_num, pad_len), dtype=int)\n",
    "for i, ann_id in enumerate(ann_ids):\n",
    "    # for j, ann_id in enumerate(ann_ids_image):\n",
    "        caption_str = coco.anns[ann_id][\"caption\"]\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption_str).lower())\n",
    "        caption = []\n",
    "        #         caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        #         caption.append(vocab('<end>'))\n",
    "        caption = np.array(caption)\n",
    "        anns[i][:len(caption)] = caption[:len(caption)]\n",
    "\n",
    "anns = to_var(torch.from_numpy(anns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "build model\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"parameters\"\"\"\n",
    "image_vector_size = 256\n",
    "embed_size = 100\n",
    "margin = 0.1\n",
    "batch_size = 100\n",
    "vocab_size = 9956\n",
    "pad_len = 62\n",
    "batch_num = sample_num//batch_size\n",
    "\n",
    "\"\"\"set model\"\"\"\n",
    "imageCNN = ImageCNN(image_vector_size=image_vector_size)\n",
    "matchCNN = MatchCNN_st(embed_size=embed_size,\n",
    "                       image_vector_size=image_vector_size,\n",
    "                       vocab_size=vocab_size,\n",
    "                       pad_len=pad_len)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    imageCNN = imageCNN.cuda()\n",
    "    matchCNN = matchCNN.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"load models\"\"\"\n",
    "model_path = \"../models\"\n",
    "imageCNN.load_state_dict(torch.load(os.path.join(model_path, 'imageCNN_st21-0.005963.pkl')))\n",
    "matchCNN.load_state_dict(torch.load(os.path.join(model_path, 'matchCNN_st21-0.005963.pkl')))\n",
    "\n",
    "# imageCNN.eval()\n",
    "# matchCNN.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"extract image feature\"\"\"\n",
    "\n",
    "# img_data = to_var(torch.zeros(sample_num, 3, 224, 224))\n",
    "img_features_batch = to_var(torch.zeros(batch_num, batch_size, image_vector_size))\n",
    "\n",
    "# for i, img in enumerate(imgs):\n",
    "#     img_data[i] = img[\"data\"]\n",
    "\n",
    "for i in range(batch_num):\n",
    "    img_features_batch[i] = imageCNN(imgs[i * batch_size:(i + 1) * batch_size])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"calculate scores\"\"\"\n",
    "scores = np.zeros((sample_num, sample_num))\n",
    "for i, caption in enumerate(anns):\n",
    "    caption_tmp = caption.unsqueeze(0)\n",
    "    caption_batch = caption_tmp.repeat(batch_size, 1)\n",
    "    for j, img_feature_batch in enumerate(img_features_batch):\n",
    "        score_batch = matchCNN(img_feature_batch, caption_batch)\n",
    "        score_batch_np = score_batch.cpu().data.numpy()\n",
    "\n",
    "        scores[j * batch_size:(j + 1) * batch_size, i] = score_batch_np[:, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"rank\"\"\"\n",
    "sorted_scores = (-scores).argsort(axis=0)\n",
    "\n",
    "scores_ranks = np.zeros((sample_num, sample_num), dtype=int)\n",
    "\n",
    "for i in range(sample_num):\n",
    "    for j in range(sample_num):\n",
    "        scores_ranks[sorted_scores[j][i]][i] = j\n",
    "\n",
    "ranks_image = np.zeros((sample_num), dtype=int)\n",
    "for i in range(sample_num):\n",
    "    ranks_image[i] = scores_ranks[i][i]\n",
    "\n",
    "# sorted_ranks_image = np.sort(ranks_image)\n",
    "# med_ranks = np.zeros(sample_num)\n",
    "# for i in range(sample_num):\n",
    "#     med_ranks[i] = sorted_ranks_image[i][0]\n",
    "\n",
    "r1 = len(ranks_image[ranks_image == 0]) / sample_num * 100\n",
    "r5 = len(ranks_image[ranks_image <= 4]) / sample_num * 100\n",
    "r10 = len(ranks_image[ranks_image <= 9]) / sample_num * 100\n",
    "med = np.mean(ranks_image)\n",
    "\n",
    "print(\"r1:\", r1)\n",
    "print(\"r5:\", r5)\n",
    "print(\"r10:\", r10)\n",
    "print(\"med:\", med)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttx",
   "language": "python",
   "name": "ttx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

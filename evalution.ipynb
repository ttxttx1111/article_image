{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from model import ImageCNN, MatchCNN\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "from data_loader import get_loader, CocoDataset\n",
    "from build_vocab import Vocabulary\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from random import shuffle\n",
    "from matchCNN_st import MatchCNN_st\n",
    "\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "\n",
    "\"\"\"load coco dataset\"\"\"\n",
    "data_dir = \"../data/coco/\"\n",
    "annotation_file = data_dir + \"annotations/captions_val2014.json\"\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# anns = coco.anns\n",
    "# imgs = coco.imgs\n",
    "\n",
    "\"\"\"extract 200 imgid and corresponding 1000 captionid\"\"\"\n",
    "sample_num = 1000\n",
    "# caption_num = sample_num * 1000\n",
    "img_ids_all = list(coco.imgs.keys())\n",
    "shuffle(img_ids_all)\n",
    "img_ids = []\n",
    "ann_ids = []\n",
    "\n",
    "for key in img_ids_all:\n",
    "    temp = coco.getAnnIds(key)\n",
    "    ann_ids.append(temp[0])\n",
    "    img_ids.append(key)\n",
    "    if len(img_ids) == sample_num:\n",
    "        break\n",
    "\n",
    "\"\"\"preprocess images\"\"\"\n",
    "image_dir = data_dir + \"resized2014/\"\n",
    "imgs = []\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\"\"\"\n",
    "imgs:\n",
    "    list of img\n",
    "img:\n",
    "    ann_ids, data, id\n",
    "\"\"\"\n",
    "for i, img_id in enumerate(img_ids):\n",
    "    img_new = {}\n",
    "    img = coco.imgs[img_id]\n",
    "    image = Image.open(image_dir + img[\"file_name\"]).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    img_new[\"ann_ids\"] = ann_ids[i]\n",
    "    img_new[\"data\"] = image\n",
    "    img_new[\"id\"] = img_id\n",
    "    imgs.append(img_new)\n",
    "\n",
    "\"\"\"preprocess annotations\"\"\"\n",
    "vocab_file = \"../data/coco/vocab.pkl\"\n",
    "pad_len = 62\n",
    "# Load vocabulary wrapper.\n",
    "with open(vocab_file, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "anns = np.zeros((sample_num, pad_len), dtype=int)\n",
    "for i, ann_id in enumerate(ann_ids):\n",
    "    # for j, ann_id in enumerate(ann_ids_image):\n",
    "        caption_str = coco.anns[ann_id][\"caption\"]\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption_str).lower())\n",
    "        caption = []\n",
    "        #         caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        #         caption.append(vocab('<end>'))\n",
    "        caption = np.array(caption)\n",
    "        anns[i][:len(tokens)] = caption[:]\n",
    "\n",
    "anns = to_var(torch.from_numpy(anns))\n",
    "\n",
    "\"\"\"parameters\"\"\"\n",
    "image_vector_size = 256\n",
    "embed_size = 100\n",
    "margin = 0.1\n",
    "batch_size = 100\n",
    "vocab_size = 9956\n",
    "momentum = 0.9\n",
    "lr = 0.0001\n",
    "pad_len = 62\n",
    "num_workers = 2\n",
    "\n",
    "\"\"\"set model\"\"\"\n",
    "imageCNN = ImageCNN(image_vector_size=image_vector_size)\n",
    "matchCNN = MatchCNN_st(embed_size=embed_size,\n",
    "                       image_vector_size=image_vector_size,\n",
    "                       vocab_size=vocab_size,\n",
    "                       pad_len=pad_len)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    imageCNN = imageCNN.cuda()\n",
    "    matchCNN = matchCNN.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"load models\"\"\"\n",
    "model_path = \"../models\"\n",
    "imageCNN.load_state_dict(torch.load(os.path.join(model_path, 'imageCNN_Nobn&drop_st180-0.005069.pkl')))\n",
    "matchCNN.load_state_dict(torch.load(os.path.join(model_path, 'matchCNN_Nobn&drop_st180-0.005069.pkl')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "cuda is available\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: sizes do not match at /pytorch/torch/lib/THC/generic/THCTensorCopy.c:51",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a9492237fa39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mimg_features_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_vector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mimg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# img_data_batch = to_var(torch.zeros(sample_num // batch_size, batch_size, 3, 224, 224))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/ttx/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mMaskedFill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSetItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/ttx/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, index, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvanced_indexing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_preprocess_adv_index_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: sizes do not match at /pytorch/torch/lib/THC/generic/THCTensorCopy.c:51"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# imageCNN = imageCNN.eval()\n",
    "# matchCNN = matchCNN.eval()\n",
    "\n",
    "\"\"\"extract image feature\"\"\"\n",
    "\n",
    "img_data = to_var(torch.zeros(sample_num, 3, 224, 224))\n",
    "img_features_batch = to_var(torch.zeros(sample_num // batch_size, batch_size, image_vector_size))\n",
    "for i, img in enumerate(imgs):\n",
    "    img_data[i] = img[\"data\"]\n",
    "\n",
    "# img_data_batch = to_var(torch.zeros(sample_num // batch_size, batch_size, 3, 224, 224))\n",
    "for i in range(sample_num // batch_size):\n",
    "    img_features_batch[i] = imageCNN(img_data[i * batch_size:(i + 1) * batch_size])\n",
    "\n",
    "scores = np.zeros((sample_num, sample_num))\n",
    "for i, caption in enumerate(anns):\n",
    "    caption_tmp = caption.unsqueeze(0)\n",
    "    caption_batch = caption_tmp.repeat(batch_size, 1)\n",
    "    for j, img_feature_batch in enumerate(img_features_batch):\n",
    "        score_batch = matchCNN(img_feature_batch, caption_batch)\n",
    "        score_batch_np = score_batch.cpu().data.numpy()\n",
    "\n",
    "        scores[j * batch_size:(j + 1) * batch_size, i] = score_batch_np[:, 0]\n",
    "\n",
    "\n",
    "# sort by column\n",
    "sorted_scores = (-scores).argsort(axis=0)\n",
    "\n",
    "scores_ranks = np.zeros((sample_num, sample_num), dtype=int)\n",
    "\n",
    "for i in range(sample_num):\n",
    "    for j in range(sample_num):\n",
    "        scores_ranks[sorted_scores[i][j]][j] = i\n",
    "\n",
    "ranks_image = np.zeros((sample_num), dtype=int)\n",
    "for i in range(sample_num):\n",
    "    ranks_image[i] = scores_ranks[i][i]\n",
    "\n",
    "# sorted_ranks_image = np.sort(ranks_image)\n",
    "# med_ranks = np.zeros(sample_num)\n",
    "# for i in range(sample_num):\n",
    "#     med_ranks[i] = sorted_ranks_image[i][0]\n",
    "\n",
    "r1 = len(ranks_image[ranks_image == 0]) / sample_num * 100\n",
    "r5 = len(ranks_image[ranks_image <= 4]) / sample_num * 100\n",
    "r10 = len(ranks_image[ranks_image <= 9]) / sample_num * 100\n",
    "med = np.mean(ranks_image)\n",
    "\n",
    "print(\"r1:\", r1)\n",
    "print(\"r5:\", r5)\n",
    "print(\"r10:\", r10)\n",
    "print(\"med:\", med)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttx",
   "language": "python",
   "name": "ttx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

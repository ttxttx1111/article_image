{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageCNN(nn.Module):\n",
    "    def __init__(self, image_vector_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(ImageCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, image_vector_size)\n",
    "        self.bn = nn.BatchNorm1d(image_vector_size, momentum=0.01)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        self.linear.weight.data.normal_(0.0, 0.02)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract the image feature vectors.\"\"\"\n",
    "        # images: batch_size * 3 * height * width\n",
    "        #  height, width is larger than 224\n",
    "        features = self.resnet(images)\n",
    "        features = Variable(features.data)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MatchCNN(nn.Module):\n",
    "    def __init__(self, embed_size, image_vector_size, vocab_size):\n",
    "        super(MatchCNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.muti_conv1_word = nn.Linear(3 * embed_size + image_vector_size, 200)\n",
    "        self.conv2_word = nn.Linear(200 * 3, 300)\n",
    "        self.conv3_word = nn.Linear(300 * 3, 300)\n",
    "        self.linear1_word = nn.Linear(600, 400)\n",
    "        self.linear2_word = nn.Linear(400, 1)\n",
    "        \n",
    "        self.conv1_phs = nn.Linear(embed_size * 3, 200)\n",
    "        self.muti_conv2_phs = nn.Linear(3 * 200 + image_vector_size, 300)\n",
    "        self.conv3_phs = nn.Linear(300 * 3, 300)\n",
    "        self.linear1_phs = nn.Linear(600, 400)\n",
    "        self.linear2_phs = nn.Linear(400, 1)\n",
    "        \n",
    "        self.conv1_phl = nn.Linear(embed_size * 3, 200)\n",
    "        self.conv2_phl = nn.Linear(200 * 3, 300)\n",
    "        self.muti_conv3_phl = nn.Linear(3 * 300 + image_vector_size, 300)\n",
    "        self.linear1_phl = nn.Linear(600, 400)\n",
    "        self.linear2_phl = nn.Linear(400, 1)\n",
    "\n",
    "        self.conv1_sen = nn.Linear(embed_size * 3, 200)\n",
    "        self.conv2_sen = nn.Linear(200 * 3, 300)\n",
    "        self.conv3_sen = nn.Linear(300 * 3, 300)\n",
    "        self.muti_linear1_sen = nn.Linear(600 + image_vector_size, 400)\n",
    "        self.linear2_sen = nn.Linear(400, 1)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "        image_vectors: batch_size * sentence_vector_size\n",
    "        sentences : batch_size * sentence_size(now fixed as 30)\n",
    "        note: Every image_vector and sentences pair should be matched\n",
    "    \"\"\"\n",
    "    def forward(self, image_vectors, sentences):\n",
    "        #For test only\n",
    "#         self.sentence_vectors = Variable(torch.randn((10, 30, 50)), requires_grad = True)\n",
    "#         image_vectors = Variable(torch.randn(10, 256))\n",
    "            \n",
    "        sentence_vectors = self.embed(sentences)\n",
    "    \n",
    "        features_word = self.conv(sentence_vectors, self.muti_conv1_word, image_vectors)\n",
    "        features_word = self.conv(features_word, self.conv2_word)\n",
    "        features_word = self.conv(features_word, self.conv3_word)\n",
    "        features_word = self.mlp(features_word, self.linear1_word, self.linear2_word)\n",
    "        \n",
    "        features_phs = self.conv(sentence_vectors, self.conv1_phs)\n",
    "        features_phs = self.conv(features_phs, self.muti_conv2_phs, image_vectors)\n",
    "        features_phs = self.conv(features_phs, self.conv3_phs)\n",
    "        features_phs = self.mlp(features_phs, self.linear1_phs, self.linear2_phs)\n",
    "        \n",
    "        features_phl = self.conv(sentence_vectors, self.conv1_phl)\n",
    "        features_phl = self.conv(features_phl, self.conv2_phl)\n",
    "        features_phl = self.conv(features_phl, self.muti_conv3_phl, image_vectors)\n",
    "        features_phl = self.mlp(features_phl, self.linear1_phl, self.linear2_phl)\n",
    "        \n",
    "        features_sen = self.conv(sentence_vectors, self.conv1_sen)\n",
    "        features_sen = self.conv(features_sen, self.conv2_sen)\n",
    "        features_sen = self.conv(features_sen, self.conv3_sen)\n",
    "        features_sen = self.mlp(features_sen, self.muti_linear1_sen, self.linear2_sen, image_vectors)\n",
    "        \n",
    "        return features_word + features_phs + features_phl + features_sen\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    features:  batch_size * sentence_size * channel_size\n",
    "    return scores: batch_size * 1\n",
    "    \"\"\"\n",
    "    def mlp(self, features, linear_function1, linear_function2, image_vectors=None):\n",
    "        features = features.contiguous()\n",
    "        features_num = self.num_flat_features(features)\n",
    "        print(\"flat size:\", features_num)\n",
    "        features = features.view(-1, features_num)\n",
    "        \n",
    "        if(image_vectors is not None):\n",
    "            features = torch.cat([features,image_vectors], dim=1)\n",
    "\n",
    "        features = F.relu(linear_function1(features))\n",
    "        features = F.relu(linear_function2(features))\n",
    "        print(\"final shape:\",features.data.numpy().shape)\n",
    "        return features\n",
    "    \n",
    "    \n",
    "#     def muti_mlp(self, features, image_vectors, linear_function1, linear_function2):\n",
    "#         features = features.contiguous()\n",
    "#         features_num = self.num_flat_features(features)\n",
    "#         print(\"flat size:\", features_num)\n",
    "\n",
    "#         features = features.view(-1, features_num)\n",
    "#         features = torch.cat([features,image_vectors], dim=1)\n",
    "#         features = F.relu(linear_function1(features))\n",
    "#         features = F.relu(linear_function2(features))\n",
    "#         print(\"final shape:\",features.data.numpy().shape)\n",
    "#         return features\n",
    "  \n",
    "\n",
    "    \"\"\"\n",
    "    includ convlution, zero_gate and pooling\n",
    "    \"\"\"\n",
    "#     def muti_conv(self, features, image_vectors, muti_conv_function):\n",
    "#         features1 = self.scan_conv(features, image_vectors)\n",
    "#         features = F.relu(muti_conv_function(features1))\n",
    "#         features = self.zero_gate(features1, features)\n",
    "#         print(\"muti_convlution1 features shape:\", features.size())\n",
    "#         features = self.sentence_pooling(features)\n",
    "#         return features;\n",
    "    \n",
    "    \n",
    "    def conv(self, features, conv_function, image_vectors = None):\n",
    "        features1 = self.scan_conv(features, image_vectors)\n",
    "        features = F.relu(conv_function(features1))\n",
    "        features = self.zero_gate(features1, features)\n",
    "        print(\"muti_convlution1 features shape:\", features.size())\n",
    "        features = self.sentence_pooling(features)\n",
    "        return features;\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    x: batch_size * feature1_size *... * featuren_size\n",
    "    return: feature1_size * feature2_size * .... featuren_size\n",
    "    \"\"\"\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  \n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    features: batch_size * sentence_size * channel_size\n",
    "    return: batch_size * sentence_size/2 * channel_size\n",
    "    \"\"\"\n",
    "    def sentence_pooling(self, features):\n",
    "        return  (F.max_pool1d(features.permute(0, 2, 1), 2)).permute(0, 2, 1)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    features: batch_size * sentence_size * channel_size\n",
    "    image_vectors: batch_size * image_size\n",
    "    sentence_image_vectors: batch_size * (sentence_size - 3 +1) * (3*channel_size + image_size)\n",
    "    \"\"\"\n",
    "#     def scan_muticonv(self, features, image_vectors):\n",
    "#         batch_size = features.size(0)\n",
    "#         sentence_size = features.size(1)\n",
    "#         channel_size = features.size(2)\n",
    "#         image_size = image_vectors.size(1)\n",
    "#         print(\"muti_convlution input features shape:\", features.size())\n",
    "# #         features_transpose = features.permute(0, 2, 1)\n",
    "        \n",
    "#         sentence_image_vectors = Variable(torch.FloatTensor(batch_size, sentence_size - 3 + 1, 3*channel_size + image_size))\n",
    "#         print(\"sentence_image_vectors shape:\", sentence_image_vectors.size())\n",
    "#         for i in range(3):\n",
    "#             sentence_image_vectors[:,:,i * channel_size:(i+1)*channel_size] = features[:,i:sentence_size - 3 + 1 + i,:]    \n",
    "#         sentence_image_vectors[:,:,3*channel_size:] = image_vectors.unsqueeze(1).repeat(1, sentence_size - 3 + 1,1)\n",
    "\n",
    "# #       features = self.muti_conv1(sentence_image_vectors)\n",
    "#         return sentence_image_vectors\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    features: batch_size * sentence_size * channel_size\n",
    "    sentence_image_vectors: batch_size * (sentence_size - 3 +1) * (3*channel_size)\n",
    "    \"\"\"\n",
    "#     def scan_conv(self, features):\n",
    "#         batch_size = features.size(0)\n",
    "#         sentence_size = features.size(1)\n",
    "#         channel_size = features.size(2)\n",
    "#         image_size = image_vectors.size(1)\n",
    "#         print(\"muti_convlution input features shape:\", features.size())\n",
    "#         #         features_transpose = features.permute(0, 2, 1)\n",
    "\n",
    "#         sentence_vectors = Variable(torch.FloatTensor(batch_size, sentence_size - 3 + 1, 3*channel_size))\n",
    "\n",
    "#         print(\"sentence_vectors shape:\", sentence_vectors.size())\n",
    "#         for i in range(3):\n",
    "#             sentence_vectors[:,:,i * channel_size:(i+1)*channel_size] = features[:,i:sentence_size - 3 + 1 + i,:]\n",
    "#         return sentence_vectors\n",
    "    def scan_conv(self, features, image_vectors=None):\n",
    "        batch_size = features.size(0)\n",
    "        sentence_size = features.size(1)\n",
    "        channel_size = features.size(2)\n",
    "        print(\"muti_convlution input features shape:\", features.size())\n",
    "        #         features_transpose = features.permute(0, 2, 1)\n",
    "        if(image_vectors is None):\n",
    "            sentence_vectors = Variable(torch.FloatTensor(batch_size, sentence_size - 3 + 1, 3*channel_size))\n",
    "        else:\n",
    "            image_size = image_vectors.size(1)\n",
    "            sentence_vectors = Variable(torch.FloatTensor(batch_size, sentence_size - 3 + 1, 3*channel_size + image_size))\n",
    "        print(\"sentence_vectors shape:\", sentence_vectors.size())\n",
    "        for i in range(3):\n",
    "            sentence_vectors[:,:,i * channel_size:(i+1)*channel_size] = features[:,i:sentence_size - 3 + 1 + i,:]\n",
    "        if(image_vectors is not None):\n",
    "            sentence_vectors[:,:,3*channel_size:] = image_vectors.unsqueeze(1).repeat(1, sentence_size - 3 + 1,1)\n",
    "        return sentence_vectors\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    if vector in feature1 is zero vectors, vector in feature should also be zero\n",
    "    \"\"\"\n",
    "    def zero_gate(self,feature1, feature2):\n",
    "        zero_vectors = feature1.sum(dim = 2, keepdim = True)\n",
    "        zero_vectors[zero_vectors > 0] = 1\n",
    "        return torch.mul(feature2, zero_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matchCNN = MatchCNN(embed_size = embed_size, image_vector_size = image_vector_size, vocab_size = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"ensemble test\"\"\"\n",
    "image_vector_size = 256\n",
    "embed_size = 50\n",
    "margin = 0.5\n",
    "batch_size = 10\n",
    "epoch = 1\n",
    "\n",
    "imageCNN = ImageCNN(image_vector_size=image_vector_size)\n",
    "matchCNN = MatchCNN(embed_size = embed_size, image_vector_size = image_vector_size, vocab_size = 1000)\n",
    "\n",
    "\n",
    "\"\"\"set optimizer\"\"\"\n",
    "# params = list(imageCNN.parameters()) + list(matchCNN.parameters())\n",
    "params = list(imageCNN.linear.parameters()) + list(imageCNN.bn.parameters()) + list(matchCNN.parameters())\n",
    "\n",
    "optimizer = optim.SGD(params, momentum=0.9, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(epoch):\n",
    "\"\"\"input data\"\"\"\n",
    "image = Variable(torch.randn(10,3,224,224))\n",
    "image_wrong = image[torch.randperm(batch_size)]\n",
    "# sentence = Variable(torch.randn(10, 30))\n",
    "sentences = Variable(torch.LongTensor(np.random.randint(low=0, high=999, size=(10,30))))\n",
    "\n",
    "\n",
    "\"\"\"extract imgae feature and embed sentence\"\"\"\n",
    "image_vectors = imageCNN(image)\n",
    "image_vectors_wrong = imageCNN(image_wrong)\n",
    "# sentence_vectors = Variable(torch.randn(10, 30, 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 406])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 600])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 900])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n",
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 150])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 856])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 900])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n",
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 150])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 600])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 1156])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n",
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 150])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 600])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 900])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n",
      "--------------------\n",
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 406])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 600])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 900])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n",
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 150])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 856])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 900])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n",
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 150])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 600])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 1156])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n",
      "muti_convlution input features shape: torch.Size([10, 30, 50])\n",
      "sentence_vectors shape: torch.Size([10, 28, 150])\n",
      "muti_convlution1 features shape: torch.Size([10, 28, 200])\n",
      "muti_convlution input features shape: torch.Size([10, 14, 200])\n",
      "sentence_vectors shape: torch.Size([10, 12, 600])\n",
      "muti_convlution1 features shape: torch.Size([10, 12, 300])\n",
      "muti_convlution input features shape: torch.Size([10, 6, 300])\n",
      "sentence_vectors shape: torch.Size([10, 4, 900])\n",
      "muti_convlution1 features shape: torch.Size([10, 4, 300])\n",
      "flat size: 600\n",
      "final shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"get correct score\"\"\"\n",
    "scores = matchCNN(image_vectors, sentences)\n",
    "print(\"-\"*20)\n",
    "scores_wrong = matchCNN(image_vectors_wrong, sentences)\n",
    "\n",
    "loss = torch.clamp(margin + scores_wrong - scores, min = 0)\n",
    "loss = torch.sum(loss)\n",
    "\n",
    "imageCNN.zero_grad()\n",
    "matchCNN.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#通过index也可以求梯度\n",
    "x = Variable(torch.ones(2,2,2), requires_grad = True)\n",
    "print(x.grad)\n",
    "\n",
    "z = Variable(torch.randn(2,2))\n",
    "z[:,:] = x[0][:][:]\n",
    "\n",
    "y = z * 3\n",
    "\n",
    "loss = torch.sum(y)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = nn.Linear(5,10)\n",
    "def test(feature, linear_func):\n",
    "    b = linear_func(feature)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = Variable(torch.ones(1,5))\n",
    "print(test(f, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(input, extra=None):\n",
    "    if(extra == None):\n",
    "        print(\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(True):\n",
    "    d = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
